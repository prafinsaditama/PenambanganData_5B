{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tugas III"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy & Information Gain\n",
    "\n",
    "Algoritma pada metode ini menggunakan konsep dari entropi. Konsep Entropi yang digunakan untuk mengukur “seberapa informatifnya” sebuah node (yang biasanya disebut seberapa baiknya).\n",
    "\n",
    "+ Entropi(S) = 0, jika semua contoh pada S berada dalam kelas yang sama.\n",
    "+ Entroiy(S) = 1, jika jumlah contoh positif dan jumlah contoh negatif dalam S adalah sama.\n",
    "+ 0 < Entropi(S) < 1, jika jumlah contoh positif dan negatif dalam S tidak sama.\n",
    "\n",
    "![](https://informatikalogi.com/wp-content/uploads/2017/07/entropi.jpg)\n",
    "\n",
    "Dimana:\n",
    "\n",
    "+ S adalah himpunan (dataset) kasus\n",
    "+ k adalah banyaknya partisi S\n",
    "+ pj adalah probabilitas yang di dapat dari Sum(Ya) dibagi Total Kasus.\n",
    "\n",
    "Setelah mendapat nilai entropi, pemilihan atribut dilakukan dengan nilai information gain terbesar.\n",
    "\n",
    "gain decision tree\n",
    "information gain\n",
    "\n",
    "Dimana:\n",
    "\n",
    "![](https://informatikalogi.com/wp-content/uploads/2017/07/gain-decision-tree.jpg)\n",
    "\n",
    "+ S = ruang (data) sample yang digunakan untuk training.\n",
    "+ A = atribut.\n",
    "+ |Si| = jumlah sample untuk nilai V.\n",
    "+ |S| = jumlah seluruh sample data.\n",
    "+ Entropi(Si) = entropy untuk sample-sample yang memiliki nilai i\n",
    "\n",
    "## Contoh\n",
    "\n",
    "Data yang telah ada pada Tabel dibawah akan digunakan untuk membentuk pohon keputusan dimana memiliku atribut-atribut seperti Cuaca, Suhu, Kelembaban, dan Berangin. Setiap atribut memiliki nilai. Sedangkan kelasnya ada pada kolom Main yaitu kelas “Tidak” dan kelas “Ya”. Kemudian data tersebut dianalisis; dataset tersebut memiliki 14 kasus yang terdiri 10 “Ya” dan 4 “Tidak” pada kolom Main.\n",
    "\n",
    "![](https://informatikalogi.com/wp-content/uploads/2017/07/Learning-Dataset-1.jpg)\n",
    "\n",
    "Kemudian hitung entropi dengan rumus seperti diatas.\n",
    "\n",
    "> Entropi (S) = (-(10/14) x log2 (10/14) + (-(4/10) x log2 (4/10)) = 0.863120569\n",
    "\n",
    "![](https://informatikalogi.com/wp-content/uploads/2017/07/Hasil-Perhitungan-pada-Dataset-1.jpg)\n",
    "<center>hasil perhitungan entropi</center>\n",
    "\n",
    "Setelah mendapatkan entropi dari keseluruhan kasus, lakukan analisis pada setiap atribut dan nilai-nilainya dan hitung entropinya.\n",
    "\n",
    "![](https://informatikalogi.com/wp-content/uploads/2017/07/Analisis-Atribut-Nilai-Banyaknya-Kejadian-Nilai-Entropi-dan-Gain-1.jpg)\n",
    "analisis atribut, nilai, banyaknya kejadian, entropi dan gain\n",
    "\n",
    "Setelah mendapatkan nilai entropy, berikutnya hitung nilai information gain dari setiap variabel.\n",
    "\n",
    "> Gain (Cuaca) = 0.863120569 –  ((4/10) x 0 + (5/14) x 0.721928095 + (5/14) x 0.970950594) = 0.258521037\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
