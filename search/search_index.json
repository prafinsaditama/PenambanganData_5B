{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands NAME : Prafinda Sutan Aditama NIM : 180411100070 MATA KULIAH : Penambangan Data_5B DOSEN PENGAMPU : MULAAB","title":"Welcome to MkDocs"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"NAME : Prafinda Sutan Aditama NIM : 180411100070 MATA KULIAH : Penambangan Data_5B DOSEN PENGAMPU : MULAAB","title":"Commands"},{"location":"BAB I/","text":"Tugas I Mean Median Mode Range Kuartil Variant Standar Deviasi","title":"Tugas I"},{"location":"BAB I/#tugas-i","text":"","title":"Tugas I"},{"location":"BAB I/#mean","text":"","title":"Mean"},{"location":"BAB I/#median","text":"","title":"Median"},{"location":"BAB I/#mode","text":"","title":"Mode"},{"location":"BAB I/#range","text":"","title":"Range"},{"location":"BAB I/#kuartil","text":"","title":"Kuartil"},{"location":"BAB I/#variant","text":"","title":"Variant"},{"location":"BAB I/#standar-deviasi","text":"","title":"Standar Deviasi"},{"location":"BAB II/","text":"Tugas II Minkowsi Distance Kelompk Minkowski diantaranya adalah Euclidean distance dan Manhattan distance, yang menjadi kasus khusus dari Minkowski distance. Manhattan Distance Manhattan distance adalah kasus khsusu dari jarak Minkowski distance pada m = 1. Seperti Minkowski Distance, Manhattan distance sensitif terhadap outlier. BIla ukuran ini digunakan dalam algoritma clustering , bentuk cluster adalah hyper-rectangular. Ukuran ini didefinisikan dengan Eucledian Distance adalah kasus khusus dari jarak Minkowski ketika m = 2. Jarak Euclidean berkinerja baik ketika digunakan untuk kumpulan data cluster kompak atau terisolasi . Meskipun jarak Euclidean sangat umum dalam pengelompokan, ia memiliki kelemahan: jika dua vektor data tidak memiliki nilai atribut yang sama, kemungkin memiliki jarak yang lebih kecil daripada pasangan vektor data lainnya yang mengandung nilai atribut yang sama. Masalah lain dengan jarak Euclidean sebagai fitur skala terbesar akan mendominasi yang lain. Normalisasi fitur kontinu adalah solusi untuk mengatasi kelemahan ini. Average Distance Berkenaan dengan kekurangan dari Jarak Euclidian Distance diatas, rata rata jarak adala versi modikfikasid ari jarak Euclidian untuk memperbaiki hasil. Untuk dua titik x,y dalam ruang dimensi n, rata-rata jarak didefinisikan dengan Weighted Average Distance Jika berdasarkan tingkatan penting dari masing masing atribut ditentukan, maka Weighted Euclidean distance adalah modifikisasi lain dari jarak Euclidean distance yang dapat digunakan. Ukuran ini dirumuskan dengan dimana wi adalah bobot yang diberikan pada atribut ke i Chord Distance Chord distance adalah salah satu ukuran jarak modifikasi Euclidean distance untuk mengatasi kekurangan dari Euclidean distance. Ini dapat dipecahkan juga dengan menggunakan skala pengukuran yang baik. Jarak ini dapat juga dihitung dari data yang tidak dinormalisasi . Chord distance didefinisikan dengan Mahalanobis distance Mahalanobis distance berdasarkan data berbeda dengan Euclidean dan Manhattan distances yang bebas antra data dengan data yang lain. Jarak Mahalanobis yang teratur dapat digunakan untuk mengekstraksi hyperellipsoidal clusters. Jarak Mahalanobis dapat mengurangi distorsi yang disebabkan oleh korelasi linier antara fitur dengan menerapkan transformasi pemutihan ke data atau dengan menggunakan kuadrat Jarak mahalanobis. Mahalanobis distance dinyatakan dengan Cosine measure Ukuran Cosine similarity lebih banyak digunakan dalam similaritas dokumen dan dinyatakan dengan Pearson correlation Pearson correlation banyak digunakan dalam data expresi gen. Ukuran similaritas ini menghitung similaritas antara duan bentuk pola expresi gen. Pearson correlation didefinisikan dengan","title":"Tugas II"},{"location":"BAB II/#tugas-ii","text":"","title":"Tugas II"},{"location":"BAB II/#minkowsi-distance","text":"Kelompk Minkowski diantaranya adalah Euclidean distance dan Manhattan distance, yang menjadi kasus khusus dari Minkowski distance.","title":"Minkowsi Distance"},{"location":"BAB II/#manhattan-distance","text":"Manhattan distance adalah kasus khsusu dari jarak Minkowski distance pada m = 1. Seperti Minkowski Distance, Manhattan distance sensitif terhadap outlier. BIla ukuran ini digunakan dalam algoritma clustering , bentuk cluster adalah hyper-rectangular. Ukuran ini didefinisikan dengan","title":"Manhattan Distance"},{"location":"BAB II/#eucledian-distance","text":"adalah kasus khusus dari jarak Minkowski ketika m = 2. Jarak Euclidean berkinerja baik ketika digunakan untuk kumpulan data cluster kompak atau terisolasi . Meskipun jarak Euclidean sangat umum dalam pengelompokan, ia memiliki kelemahan: jika dua vektor data tidak memiliki nilai atribut yang sama, kemungkin memiliki jarak yang lebih kecil daripada pasangan vektor data lainnya yang mengandung nilai atribut yang sama. Masalah lain dengan jarak Euclidean sebagai fitur skala terbesar akan mendominasi yang lain. Normalisasi fitur kontinu adalah solusi untuk mengatasi kelemahan ini.","title":"Eucledian Distance"},{"location":"BAB II/#average-distance","text":"Berkenaan dengan kekurangan dari Jarak Euclidian Distance diatas, rata rata jarak adala versi modikfikasid ari jarak Euclidian untuk memperbaiki hasil. Untuk dua titik x,y dalam ruang dimensi n, rata-rata jarak didefinisikan dengan","title":"Average Distance"},{"location":"BAB II/#weighted-average-distance","text":"Jika berdasarkan tingkatan penting dari masing masing atribut ditentukan, maka Weighted Euclidean distance adalah modifikisasi lain dari jarak Euclidean distance yang dapat digunakan. Ukuran ini dirumuskan dengan dimana wi adalah bobot yang diberikan pada atribut ke i","title":"Weighted Average Distance"},{"location":"BAB II/#chord-distance","text":"Chord distance adalah salah satu ukuran jarak modifikasi Euclidean distance untuk mengatasi kekurangan dari Euclidean distance. Ini dapat dipecahkan juga dengan menggunakan skala pengukuran yang baik. Jarak ini dapat juga dihitung dari data yang tidak dinormalisasi . Chord distance didefinisikan dengan","title":"Chord Distance"},{"location":"BAB II/#mahalanobis-distance","text":"Mahalanobis distance berdasarkan data berbeda dengan Euclidean dan Manhattan distances yang bebas antra data dengan data yang lain. Jarak Mahalanobis yang teratur dapat digunakan untuk mengekstraksi hyperellipsoidal clusters. Jarak Mahalanobis dapat mengurangi distorsi yang disebabkan oleh korelasi linier antara fitur dengan menerapkan transformasi pemutihan ke data atau dengan menggunakan kuadrat Jarak mahalanobis. Mahalanobis distance dinyatakan dengan","title":"Mahalanobis distance"},{"location":"BAB II/#cosine-measure","text":"Ukuran Cosine similarity lebih banyak digunakan dalam similaritas dokumen dan dinyatakan dengan","title":"Cosine measure"},{"location":"BAB II/#pearson-correlation","text":"Pearson correlation banyak digunakan dalam data expresi gen. Ukuran similaritas ini menghitung similaritas antara duan bentuk pola expresi gen. Pearson correlation didefinisikan dengan","title":"Pearson correlation"},{"location":"BAB III/","text":"Tugas III Entropy & Information Gain Algoritma pada metode ini menggunakan konsep dari entropi. Konsep Entropi yang digunakan untuk mengukur \u201cseberapa informatifnya\u201d sebuah node (yang biasanya disebut seberapa baiknya). Entropi(S) = 0, jika semua contoh pada S berada dalam kelas yang sama. Entroiy(S) = 1, jika jumlah contoh positif dan jumlah contoh negatif dalam S adalah sama. 0 < Entropi(S) < 1, jika jumlah contoh positif dan negatif dalam S tidak sama. Dimana: S adalah himpunan (dataset) kasus k adalah banyaknya partisi S pj adalah probabilitas yang di dapat dari Sum(Ya) dibagi Total Kasus. Setelah mendapat nilai entropi, pemilihan atribut dilakukan dengan nilai information gain terbesar. gain decision tree information gain Dimana: S = ruang (data) sample yang digunakan untuk training. A = atribut. |Si| = jumlah sample untuk nilai V. |S| = jumlah seluruh sample data. Entropi(Si) = entropy untuk sample-sample yang memiliki nilai i Contoh Data yang telah ada pada Tabel dibawah akan digunakan untuk membentuk pohon keputusan dimana memiliku atribut-atribut seperti Cuaca, Suhu, Kelembaban, dan Berangin. Setiap atribut memiliki nilai. Sedangkan kelasnya ada pada kolom Main yaitu kelas \u201cTidak\u201d dan kelas \u201cYa\u201d. Kemudian data tersebut dianalisis; dataset tersebut memiliki 14 kasus yang terdiri 10 \u201cYa\u201d dan 4 \u201cTidak\u201d pada kolom Main. Kemudian hitung entropi dengan rumus seperti diatas. Entropi (S) = (-(10/14) x log2 (10/14) + (-(4/10) x log2 (4/10)) = 0.863120569 hasil perhitungan entropi Setelah mendapatkan entropi dari keseluruhan kasus, lakukan analisis pada setiap atribut dan nilai-nilainya dan hitung entropinya. analisis atribut, nilai, banyaknya kejadian, entropi dan gain Setelah mendapatkan nilai entropy, berikutnya hitung nilai information gain dari setiap variabel. Gain (Cuaca) = 0.863120569 \u2013 ((4/10) x 0 + (5/14) x 0.721928095 + (5/14) x 0.970950594) = 0.258521037 Implementasi from pandas import* from math import log from sklearn.feature_selection import mutual_info_classif df = read_csv(\"BAB III.csv\", sep=';') df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Cuaca Suhu Kelembapan Berangin Main 0 Cerah Panas Tinggi Salah Tidak 1 Cerah Panas Tinggi Benar Tidak 2 Berawan Panas Tinggi Salah Ya 3 Hujan Sejuk Tinggi Salah Ya 4 Hujan Dingin Normal Salah Ya 5 Hujan Dingin Normal Benar Ya 6 Berawan Dingin Normal Benar Ya 7 Cerah Sejuk Tinggi Salah Tidak 8 Cerah Dingin Normal Salah Ya 9 Hujan Sejuk Normal Salah Ya 10 Cerah Sejuk Normal Benar Ya 11 Berawan Sejuk Tinggi Benar Ya 12 Berawan Panas Normal Salah Ya 13 Hujan Sejuk Tinggi Benar Tidak def findEntropy(column): rawGroups = df.groupby(column) targetGroups = [[key, len(data), len(data)/df[column].size] for key,data in rawGroups] targetGroups = DataFrame(targetGroups, columns=['Data', 'N', 'Peluang']) return sum([-x*log(x,2) for x in targetGroups['Peluang']]), targetGroups, rawGroups entropyTarget, groupTargets, _ = findEntropy('Main') print('entropy target =', entropyTarget) groupTargets entropy target = 0.863120568566631 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Data N Peluang 0 Tidak 4 0.285714 1 Ya 10 0.714286 def findGain(column): entropyOutlook, groupOutlooks, rawOutlooks = findEntropy(column) print(groupOutlooks) gain = entropyTarget-sum(len(data)/len(df)*sum(-x/len(data)*log(x/len(data),2) for x in data.groupby('Main').size()) for key,data in rawOutlooks) print(\"gain\",column,\"=\",gain) print() return gain gains = [[x,findGain(x)] for x in ['Cuaca','Suhu','Kelembapan','Berangin']] DataFrame(gains) Data N Peluang 0 Berawan 4 0.285714 1 Cerah 5 0.357143 2 Hujan 5 0.357143 gain Cuaca = 0.2585210366587628 Data N Peluang 0 Dingin 4 0.285714 1 Panas 4 0.285714 2 Sejuk 6 0.428571 gain Suhu = 0.18385092540042125 Data N Peluang 0 Normal 7 0.5 1 Tinggi 7 0.5 gain Kelembapan = 0.3705065005495052 Data N Peluang 0 Benar 6 0.428571 1 Salah 8 0.571429 gain Berangin = 0.0059777114237739015 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 0 Cuaca 0.258521 1 Suhu 0.183851 2 Kelembapan 0.370507 3 Berangin 0.005978","title":"Tugas III"},{"location":"BAB III/#tugas-iii","text":"","title":"Tugas III"},{"location":"BAB III/#entropy-information-gain","text":"Algoritma pada metode ini menggunakan konsep dari entropi. Konsep Entropi yang digunakan untuk mengukur \u201cseberapa informatifnya\u201d sebuah node (yang biasanya disebut seberapa baiknya). Entropi(S) = 0, jika semua contoh pada S berada dalam kelas yang sama. Entroiy(S) = 1, jika jumlah contoh positif dan jumlah contoh negatif dalam S adalah sama. 0 < Entropi(S) < 1, jika jumlah contoh positif dan negatif dalam S tidak sama. Dimana: S adalah himpunan (dataset) kasus k adalah banyaknya partisi S pj adalah probabilitas yang di dapat dari Sum(Ya) dibagi Total Kasus. Setelah mendapat nilai entropi, pemilihan atribut dilakukan dengan nilai information gain terbesar. gain decision tree information gain Dimana: S = ruang (data) sample yang digunakan untuk training. A = atribut. |Si| = jumlah sample untuk nilai V. |S| = jumlah seluruh sample data. Entropi(Si) = entropy untuk sample-sample yang memiliki nilai i","title":"Entropy &amp; Information Gain"},{"location":"BAB III/#contoh","text":"Data yang telah ada pada Tabel dibawah akan digunakan untuk membentuk pohon keputusan dimana memiliku atribut-atribut seperti Cuaca, Suhu, Kelembaban, dan Berangin. Setiap atribut memiliki nilai. Sedangkan kelasnya ada pada kolom Main yaitu kelas \u201cTidak\u201d dan kelas \u201cYa\u201d. Kemudian data tersebut dianalisis; dataset tersebut memiliki 14 kasus yang terdiri 10 \u201cYa\u201d dan 4 \u201cTidak\u201d pada kolom Main. Kemudian hitung entropi dengan rumus seperti diatas. Entropi (S) = (-(10/14) x log2 (10/14) + (-(4/10) x log2 (4/10)) = 0.863120569 hasil perhitungan entropi Setelah mendapatkan entropi dari keseluruhan kasus, lakukan analisis pada setiap atribut dan nilai-nilainya dan hitung entropinya. analisis atribut, nilai, banyaknya kejadian, entropi dan gain Setelah mendapatkan nilai entropy, berikutnya hitung nilai information gain dari setiap variabel. Gain (Cuaca) = 0.863120569 \u2013 ((4/10) x 0 + (5/14) x 0.721928095 + (5/14) x 0.970950594) = 0.258521037","title":"Contoh"},{"location":"BAB III/#implementasi","text":"from pandas import* from math import log from sklearn.feature_selection import mutual_info_classif df = read_csv(\"BAB III.csv\", sep=';') df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Cuaca Suhu Kelembapan Berangin Main 0 Cerah Panas Tinggi Salah Tidak 1 Cerah Panas Tinggi Benar Tidak 2 Berawan Panas Tinggi Salah Ya 3 Hujan Sejuk Tinggi Salah Ya 4 Hujan Dingin Normal Salah Ya 5 Hujan Dingin Normal Benar Ya 6 Berawan Dingin Normal Benar Ya 7 Cerah Sejuk Tinggi Salah Tidak 8 Cerah Dingin Normal Salah Ya 9 Hujan Sejuk Normal Salah Ya 10 Cerah Sejuk Normal Benar Ya 11 Berawan Sejuk Tinggi Benar Ya 12 Berawan Panas Normal Salah Ya 13 Hujan Sejuk Tinggi Benar Tidak def findEntropy(column): rawGroups = df.groupby(column) targetGroups = [[key, len(data), len(data)/df[column].size] for key,data in rawGroups] targetGroups = DataFrame(targetGroups, columns=['Data', 'N', 'Peluang']) return sum([-x*log(x,2) for x in targetGroups['Peluang']]), targetGroups, rawGroups entropyTarget, groupTargets, _ = findEntropy('Main') print('entropy target =', entropyTarget) groupTargets entropy target = 0.863120568566631 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Data N Peluang 0 Tidak 4 0.285714 1 Ya 10 0.714286 def findGain(column): entropyOutlook, groupOutlooks, rawOutlooks = findEntropy(column) print(groupOutlooks) gain = entropyTarget-sum(len(data)/len(df)*sum(-x/len(data)*log(x/len(data),2) for x in data.groupby('Main').size()) for key,data in rawOutlooks) print(\"gain\",column,\"=\",gain) print() return gain gains = [[x,findGain(x)] for x in ['Cuaca','Suhu','Kelembapan','Berangin']] DataFrame(gains) Data N Peluang 0 Berawan 4 0.285714 1 Cerah 5 0.357143 2 Hujan 5 0.357143 gain Cuaca = 0.2585210366587628 Data N Peluang 0 Dingin 4 0.285714 1 Panas 4 0.285714 2 Sejuk 6 0.428571 gain Suhu = 0.18385092540042125 Data N Peluang 0 Normal 7 0.5 1 Tinggi 7 0.5 gain Kelembapan = 0.3705065005495052 Data N Peluang 0 Benar 6 0.428571 1 Salah 8 0.571429 gain Berangin = 0.0059777114237739015 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 0 Cuaca 0.258521 1 Suhu 0.183851 2 Kelembapan 0.370507 3 Berangin 0.005978","title":"Implementasi"},{"location":"BAB IV/","text":"Tugas IV Naive Bayes Algoritma Naive Bayes merupakan sebuah metoda klasifikasi menggunakan metode probabilitas dan statistik yg dikemukakan oleh ilmuwan Inggris Thomas Bayes. Algoritma Naive Bayes memprediksi peluang di masa depan berdasarkan pengalaman di masa sebelumnya sehingga dikenal sebagai Teorema Bayes. Ciri utama dr Na\u00efve Bayes Classifier ini adalah asumsi yg sangat kuat (na\u00eff) akan independensi dari masing-masing kondisi / kejadian. Naive Bayes Classifier bekerja sangat baik dibanding dengan model classifier lainnya. Hal ini dibuktikan pada jurnal Xhemali, Daniela, Chris J. Hinde, and Roger G. Stone. \u201cNaive Bayes vs. decision trees vs. neural networks in the classification of training web pages.\u201d (2009), mengatakan bahwa \u201cNa\u00efve Bayes Classifier memiliki tingkat akurasi yg lebih baik dibanding model classifier lainnya\u201d. Keuntungan penggunan adalah bahwa metoda ini hanya membutuhkan jumlah data pelatihan (training data) yang kecil untuk menentukan estimasi parameter yg diperlukan dalam proses pengklasifikasian. Karena yg diasumsikan sebagai variabel independent, maka hanya varians dari suatu variabel dalam sebuah kelas yang dibutuhkan untuk menentukan klasifikasi, bukan keseluruhan dari matriks kovarians. Tahapan dari proses algoritma Naive Bayes adalah: Menghitung jumlah kelas / label. Menghitung Jumlah Kasus Per Kelas Kalikan Semua Variable Kelas Bandingkan Hasil Per Kelas Keterangan : + x : Data dengan class yang belum diketahui + c : Hipotesis data merupakan suatu class spesifik + P(c|x) : Probabilitas hipotesis berdasar kondisi (posteriori probability) + P(c) : Probabilitas hipotesis (prior probability) + P(x|c) : Probabilitas berdasarkan kondisi pada hipotesis + P(x) : Probabilitas c Rumus diatas menjelaskan bahwa peluang masuknya sampel karakteristik tertentu dalam kelas C (Posterior) adalah peluang munculnya kelas C (sebelum masuknya sampel tersebut, seringkali disebut prior), dikali dengan peluang kemunculan karakteristik karakteristik sampel pada kelas C (disebut juga likelihood), dibagi dengan peluang kemunculan karakteristik sampel secara global ( disebut juga evidence). Karena itu, rumus diatas dapat pula ditulis sebagai berikut : Nilai Evidence selalu tetap untuk setiap kelas pada satu sampel. Nilai dari posterior tersebut nantinya akan dibandingkan dengan nilai nilai posterior kelas lainnya untuk menentukan ke kelas apa suatu sampel akan diklasifikasikan. Penjabaran lebih lanjut rumus Bayes tersebut dilakukan dengan menjabarkan (c|x1,\u2026,xn) menggunakan aturan perkalian sebagai berikut : Dapat dilihat bahwa hasil penjabaran tersebut menyebabkan semakin banyak dan semakin kompleksnya faktor faktor syarat yang mempengaruhi nilai probabilitas, yang hampir mustahil untuk dianalisa satu persatu. Akibatnya, perhitungan tersebut menjadi sulit untuk dilakukan. Disinilah digunakan asumsi independensi yang sangat tinggi (naif), bahwa masing masing petunjuk saling bebas (independen) satu sama lain. Dengan asumsi tersebut, maka berlaku suatu kesamaan sebagai berikut: Persamaan diatas merupakan model dari Teorema Naive Bayes yang selanjutnya akan digunakan dalam proses klasifikasi. Untuk klasifikasi dengan data kontinyu digunakan rumus Densitas Gauss : Keterangan : + P : Peluang + Xi : Atribut ke i + xi : Nilai atribut ke i + Y : Kelas yang dicari + yj : Sub kelas Y yang dicari + u : Mean, menyatakan rata rata dari seluruh atribut + o : Deviasi standar, menyatakan varian dari seluruh atribut #Contoh eksekusi from sklearn import datasets, model_selection from pandas import * from sklearn.naive_bayes import GaussianNB from sklearn.metrics import classification_report, confusion_matrix, accuracy_score iris = datasets.load_iris() data = [list(s)+[iris.target_names[iris.target[i]]] for i,s in enumerate(iris.data)] dataset = DataFrame(data, columns=iris.feature_names+['class']) array = dataset.values X = array[:,0:4] Y = array[:,4] X_train, X_validation, Y_train, Y_validation = \\ model_selection.train_test_split(X, Y, \\ train_size=0.2, random_state=2) classifier = GaussianNB() classifier.fit(X_train, Y_train) p = classifier.predict(X_validation) print(\"Akurasi: \", accuracy_score(Y_validation, p)) print(classification_report(Y_validation, p)) Akurasi: 0.8833333333333333 precision recall f1-score support setosa 1.00 1.00 1.00 39 versicolor 0.75 0.97 0.84 39 virginica 0.97 0.69 0.81 42 accuracy 0.88 120 macro avg 0.90 0.89 0.88 120 weighted avg 0.91 0.88 0.88 120","title":"Tugas IV"},{"location":"BAB IV/#tugas-iv","text":"","title":"Tugas IV"},{"location":"BAB IV/#naive-bayes","text":"Algoritma Naive Bayes merupakan sebuah metoda klasifikasi menggunakan metode probabilitas dan statistik yg dikemukakan oleh ilmuwan Inggris Thomas Bayes. Algoritma Naive Bayes memprediksi peluang di masa depan berdasarkan pengalaman di masa sebelumnya sehingga dikenal sebagai Teorema Bayes. Ciri utama dr Na\u00efve Bayes Classifier ini adalah asumsi yg sangat kuat (na\u00eff) akan independensi dari masing-masing kondisi / kejadian. Naive Bayes Classifier bekerja sangat baik dibanding dengan model classifier lainnya. Hal ini dibuktikan pada jurnal Xhemali, Daniela, Chris J. Hinde, and Roger G. Stone. \u201cNaive Bayes vs. decision trees vs. neural networks in the classification of training web pages.\u201d (2009), mengatakan bahwa \u201cNa\u00efve Bayes Classifier memiliki tingkat akurasi yg lebih baik dibanding model classifier lainnya\u201d. Keuntungan penggunan adalah bahwa metoda ini hanya membutuhkan jumlah data pelatihan (training data) yang kecil untuk menentukan estimasi parameter yg diperlukan dalam proses pengklasifikasian. Karena yg diasumsikan sebagai variabel independent, maka hanya varians dari suatu variabel dalam sebuah kelas yang dibutuhkan untuk menentukan klasifikasi, bukan keseluruhan dari matriks kovarians. Tahapan dari proses algoritma Naive Bayes adalah: Menghitung jumlah kelas / label. Menghitung Jumlah Kasus Per Kelas Kalikan Semua Variable Kelas Bandingkan Hasil Per Kelas Keterangan : + x : Data dengan class yang belum diketahui + c : Hipotesis data merupakan suatu class spesifik + P(c|x) : Probabilitas hipotesis berdasar kondisi (posteriori probability) + P(c) : Probabilitas hipotesis (prior probability) + P(x|c) : Probabilitas berdasarkan kondisi pada hipotesis + P(x) : Probabilitas c Rumus diatas menjelaskan bahwa peluang masuknya sampel karakteristik tertentu dalam kelas C (Posterior) adalah peluang munculnya kelas C (sebelum masuknya sampel tersebut, seringkali disebut prior), dikali dengan peluang kemunculan karakteristik karakteristik sampel pada kelas C (disebut juga likelihood), dibagi dengan peluang kemunculan karakteristik sampel secara global ( disebut juga evidence). Karena itu, rumus diatas dapat pula ditulis sebagai berikut : Nilai Evidence selalu tetap untuk setiap kelas pada satu sampel. Nilai dari posterior tersebut nantinya akan dibandingkan dengan nilai nilai posterior kelas lainnya untuk menentukan ke kelas apa suatu sampel akan diklasifikasikan. Penjabaran lebih lanjut rumus Bayes tersebut dilakukan dengan menjabarkan (c|x1,\u2026,xn) menggunakan aturan perkalian sebagai berikut : Dapat dilihat bahwa hasil penjabaran tersebut menyebabkan semakin banyak dan semakin kompleksnya faktor faktor syarat yang mempengaruhi nilai probabilitas, yang hampir mustahil untuk dianalisa satu persatu. Akibatnya, perhitungan tersebut menjadi sulit untuk dilakukan. Disinilah digunakan asumsi independensi yang sangat tinggi (naif), bahwa masing masing petunjuk saling bebas (independen) satu sama lain. Dengan asumsi tersebut, maka berlaku suatu kesamaan sebagai berikut: Persamaan diatas merupakan model dari Teorema Naive Bayes yang selanjutnya akan digunakan dalam proses klasifikasi. Untuk klasifikasi dengan data kontinyu digunakan rumus Densitas Gauss : Keterangan : + P : Peluang + Xi : Atribut ke i + xi : Nilai atribut ke i + Y : Kelas yang dicari + yj : Sub kelas Y yang dicari + u : Mean, menyatakan rata rata dari seluruh atribut + o : Deviasi standar, menyatakan varian dari seluruh atribut #Contoh eksekusi from sklearn import datasets, model_selection from pandas import * from sklearn.naive_bayes import GaussianNB from sklearn.metrics import classification_report, confusion_matrix, accuracy_score iris = datasets.load_iris() data = [list(s)+[iris.target_names[iris.target[i]]] for i,s in enumerate(iris.data)] dataset = DataFrame(data, columns=iris.feature_names+['class']) array = dataset.values X = array[:,0:4] Y = array[:,4] X_train, X_validation, Y_train, Y_validation = \\ model_selection.train_test_split(X, Y, \\ train_size=0.2, random_state=2) classifier = GaussianNB() classifier.fit(X_train, Y_train) p = classifier.predict(X_validation) print(\"Akurasi: \", accuracy_score(Y_validation, p)) print(classification_report(Y_validation, p)) Akurasi: 0.8833333333333333 precision recall f1-score support setosa 1.00 1.00 1.00 39 versicolor 0.75 0.97 0.84 39 virginica 0.97 0.69 0.81 42 accuracy 0.88 120 macro avg 0.90 0.89 0.88 120 weighted avg 0.91 0.88 0.88 120","title":"Naive Bayes"}]}