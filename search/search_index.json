{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands NAME : Prafinda Sutan Aditama NIM : 180411100070 MATA KULIAH : Penambangan Data_5B DOSEN PENGAMPU : MULAAB","title":"Welcome to MkDocs"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"NAME : Prafinda Sutan Aditama NIM : 180411100070 MATA KULIAH : Penambangan Data_5B DOSEN PENGAMPU : MULAAB","title":"Commands"},{"location":"BAB I/","text":"Tugas I Mean Median Mode Range Kuartil Variant Standar Deviasi","title":"Tugas I"},{"location":"BAB I/#tugas-i","text":"","title":"Tugas I"},{"location":"BAB I/#mean","text":"","title":"Mean"},{"location":"BAB I/#median","text":"","title":"Median"},{"location":"BAB I/#mode","text":"","title":"Mode"},{"location":"BAB I/#range","text":"","title":"Range"},{"location":"BAB I/#kuartil","text":"","title":"Kuartil"},{"location":"BAB I/#variant","text":"","title":"Variant"},{"location":"BAB I/#standar-deviasi","text":"","title":"Standar Deviasi"},{"location":"BAB II/","text":"Tugas II Minkowsi Manhattan Eucledian Pearson","title":"Tugas II"},{"location":"BAB II/#tugas-ii","text":"","title":"Tugas II"},{"location":"BAB II/#minkowsi","text":"","title":"Minkowsi"},{"location":"BAB II/#manhattan","text":"","title":"Manhattan"},{"location":"BAB II/#eucledian","text":"","title":"Eucledian"},{"location":"BAB II/#pearson","text":"","title":"Pearson"},{"location":"BAB III/","text":"Tugas III Entropy & Information Gain Algoritma pada metode ini menggunakan konsep dari entropi. Konsep Entropi yang digunakan untuk mengukur \u201cseberapa informatifnya\u201d sebuah node (yang biasanya disebut seberapa baiknya). Entropi(S) = 0, jika semua contoh pada S berada dalam kelas yang sama. Entroiy(S) = 1, jika jumlah contoh positif dan jumlah contoh negatif dalam S adalah sama. 0 < Entropi(S) < 1, jika jumlah contoh positif dan negatif dalam S tidak sama. Dimana: S adalah himpunan (dataset) kasus k adalah banyaknya partisi S pj adalah probabilitas yang di dapat dari Sum(Ya) dibagi Total Kasus. Setelah mendapat nilai entropi, pemilihan atribut dilakukan dengan nilai information gain terbesar. gain decision tree information gain Dimana: S = ruang (data) sample yang digunakan untuk training. A = atribut. |Si| = jumlah sample untuk nilai V. |S| = jumlah seluruh sample data. Entropi(Si) = entropy untuk sample-sample yang memiliki nilai i Contoh Data yang telah ada pada Tabel dibawah akan digunakan untuk membentuk pohon keputusan dimana memiliku atribut-atribut seperti Cuaca, Suhu, Kelembaban, dan Berangin. Setiap atribut memiliki nilai. Sedangkan kelasnya ada pada kolom Main yaitu kelas \u201cTidak\u201d dan kelas \u201cYa\u201d. Kemudian data tersebut dianalisis; dataset tersebut memiliki 14 kasus yang terdiri 10 \u201cYa\u201d dan 4 \u201cTidak\u201d pada kolom Main. Kemudian hitung entropi dengan rumus seperti diatas. Entropi (S) = (-(10/14) x log2 (10/14) + (-(4/10) x log2 (4/10)) = 0.863120569 hasil perhitungan entropi Setelah mendapatkan entropi dari keseluruhan kasus, lakukan analisis pada setiap atribut dan nilai-nilainya dan hitung entropinya. analisis atribut, nilai, banyaknya kejadian, entropi dan gain Setelah mendapatkan nilai entropy, berikutnya hitung nilai information gain dari setiap variabel. Gain (Cuaca) = 0.863120569 \u2013 ((4/10) x 0 + (5/14) x 0.721928095 + (5/14) x 0.970950594) = 0.258521037","title":"Tugas III"},{"location":"BAB III/#tugas-iii","text":"","title":"Tugas III"},{"location":"BAB III/#entropy-information-gain","text":"Algoritma pada metode ini menggunakan konsep dari entropi. Konsep Entropi yang digunakan untuk mengukur \u201cseberapa informatifnya\u201d sebuah node (yang biasanya disebut seberapa baiknya). Entropi(S) = 0, jika semua contoh pada S berada dalam kelas yang sama. Entroiy(S) = 1, jika jumlah contoh positif dan jumlah contoh negatif dalam S adalah sama. 0 < Entropi(S) < 1, jika jumlah contoh positif dan negatif dalam S tidak sama. Dimana: S adalah himpunan (dataset) kasus k adalah banyaknya partisi S pj adalah probabilitas yang di dapat dari Sum(Ya) dibagi Total Kasus. Setelah mendapat nilai entropi, pemilihan atribut dilakukan dengan nilai information gain terbesar. gain decision tree information gain Dimana: S = ruang (data) sample yang digunakan untuk training. A = atribut. |Si| = jumlah sample untuk nilai V. |S| = jumlah seluruh sample data. Entropi(Si) = entropy untuk sample-sample yang memiliki nilai i","title":"Entropy &amp; Information Gain"},{"location":"BAB III/#contoh","text":"Data yang telah ada pada Tabel dibawah akan digunakan untuk membentuk pohon keputusan dimana memiliku atribut-atribut seperti Cuaca, Suhu, Kelembaban, dan Berangin. Setiap atribut memiliki nilai. Sedangkan kelasnya ada pada kolom Main yaitu kelas \u201cTidak\u201d dan kelas \u201cYa\u201d. Kemudian data tersebut dianalisis; dataset tersebut memiliki 14 kasus yang terdiri 10 \u201cYa\u201d dan 4 \u201cTidak\u201d pada kolom Main. Kemudian hitung entropi dengan rumus seperti diatas. Entropi (S) = (-(10/14) x log2 (10/14) + (-(4/10) x log2 (4/10)) = 0.863120569 hasil perhitungan entropi Setelah mendapatkan entropi dari keseluruhan kasus, lakukan analisis pada setiap atribut dan nilai-nilainya dan hitung entropinya. analisis atribut, nilai, banyaknya kejadian, entropi dan gain Setelah mendapatkan nilai entropy, berikutnya hitung nilai information gain dari setiap variabel. Gain (Cuaca) = 0.863120569 \u2013 ((4/10) x 0 + (5/14) x 0.721928095 + (5/14) x 0.970950594) = 0.258521037","title":"Contoh"}]}