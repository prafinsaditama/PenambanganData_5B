{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands NAME : Prafinda Sutan Aditama NIM : 180411100070 MATA KULIAH : Penambangan Data_5B DOSEN PENGAMPU : MULAAB","title":"Welcome to MkDocs"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"NAME : Prafinda Sutan Aditama NIM : 180411100070 MATA KULIAH : Penambangan Data_5B DOSEN PENGAMPU : MULAAB","title":"Commands"},{"location":"BAB I/","text":"Tugas I Mean Median Mode Range Kuartil Variant Standar Deviasi","title":"Tugas I"},{"location":"BAB I/#tugas-i","text":"","title":"Tugas I"},{"location":"BAB I/#mean","text":"","title":"Mean"},{"location":"BAB I/#median","text":"","title":"Median"},{"location":"BAB I/#mode","text":"","title":"Mode"},{"location":"BAB I/#range","text":"","title":"Range"},{"location":"BAB I/#kuartil","text":"","title":"Kuartil"},{"location":"BAB I/#variant","text":"","title":"Variant"},{"location":"BAB I/#standar-deviasi","text":"","title":"Standar Deviasi"},{"location":"BAB II/","text":"Tugas II Minkowsi Manhattan Eucledian Pearson","title":"Tugas II"},{"location":"BAB II/#tugas-ii","text":"","title":"Tugas II"},{"location":"BAB II/#minkowsi","text":"","title":"Minkowsi"},{"location":"BAB II/#manhattan","text":"","title":"Manhattan"},{"location":"BAB II/#eucledian","text":"","title":"Eucledian"},{"location":"BAB II/#pearson","text":"","title":"Pearson"},{"location":"BAB III/","text":"Tugas III Entropy & Information Gain Algoritma pada metode ini menggunakan konsep dari entropi. Konsep Entropi yang digunakan untuk mengukur \u201cseberapa informatifnya\u201d sebuah node (yang biasanya disebut seberapa baiknya). Entropi(S) = 0, jika semua contoh pada S berada dalam kelas yang sama. Entroiy(S) = 1, jika jumlah contoh positif dan jumlah contoh negatif dalam S adalah sama. 0 < Entropi(S) < 1, jika jumlah contoh positif dan negatif dalam S tidak sama. Dimana: S adalah himpunan (dataset) kasus k adalah banyaknya partisi S pj adalah probabilitas yang di dapat dari Sum(Ya) dibagi Total Kasus. Setelah mendapat nilai entropi, pemilihan atribut dilakukan dengan nilai information gain terbesar. gain decision tree information gain Dimana: S = ruang (data) sample yang digunakan untuk training. A = atribut. |Si| = jumlah sample untuk nilai V. |S| = jumlah seluruh sample data. Entropi(Si) = entropy untuk sample-sample yang memiliki nilai i Contoh Data yang telah ada pada Tabel dibawah akan digunakan untuk membentuk pohon keputusan dimana memiliku atribut-atribut seperti Cuaca, Suhu, Kelembaban, dan Berangin. Setiap atribut memiliki nilai. Sedangkan kelasnya ada pada kolom Main yaitu kelas \u201cTidak\u201d dan kelas \u201cYa\u201d. Kemudian data tersebut dianalisis; dataset tersebut memiliki 14 kasus yang terdiri 10 \u201cYa\u201d dan 4 \u201cTidak\u201d pada kolom Main. Kemudian hitung entropi dengan rumus seperti diatas. Entropi (S) = (-(10/14) x log2 (10/14) + (-(4/10) x log2 (4/10)) = 0.863120569 hasil perhitungan entropi Setelah mendapatkan entropi dari keseluruhan kasus, lakukan analisis pada setiap atribut dan nilai-nilainya dan hitung entropinya. analisis atribut, nilai, banyaknya kejadian, entropi dan gain Setelah mendapatkan nilai entropy, berikutnya hitung nilai information gain dari setiap variabel. Gain (Cuaca) = 0.863120569 \u2013 ((4/10) x 0 + (5/14) x 0.721928095 + (5/14) x 0.970950594) = 0.258521037 Implementasi from pandas import* from math import log from sklearn.feature_selection import mutual_info_classif df = read_csv(\"BAB III.csv\", sep=';') df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Cuaca Suhu Kelembapan Berangin Main 0 Cerah Panas Tinggi Salah Tidak 1 Cerah Panas Tinggi Benar Tidak 2 Berawan Panas Tinggi Salah Ya 3 Hujan Sejuk Tinggi Salah Ya 4 Hujan Dingin Normal Salah Ya 5 Hujan Dingin Normal Benar Ya 6 Berawan Dingin Normal Benar Ya 7 Cerah Sejuk Tinggi Salah Tidak 8 Cerah Dingin Normal Salah Ya 9 Hujan Sejuk Normal Salah Ya 10 Cerah Sejuk Normal Benar Ya 11 Berawan Sejuk Tinggi Benar Ya 12 Berawan Panas Normal Salah Ya 13 Hujan Sejuk Tinggi Benar Tidak def findEntropy(column): rawGroups = df.groupby(column) targetGroups = [[key, len(data), len(data)/df[column].size] for key,data in rawGroups] targetGroups = DataFrame(targetGroups, columns=['Data', 'N', 'Peluang']) return sum([-x*log(x,2) for x in targetGroups['Peluang']]), targetGroups, rawGroups entropyTarget, groupTargets, _ = findEntropy('Main') print('entropy target =', entropyTarget) groupTargets entropy target = 0.863120568566631 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Data N Peluang 0 Tidak 4 0.285714 1 Ya 10 0.714286 def findGain(column): entropyOutlook, groupOutlooks, rawOutlooks = findEntropy(column) print(groupOutlooks) gain = entropyTarget-sum(len(data)/len(df)*sum(-x/len(data)*log(x/len(data),2) for x in data.groupby('Main').size()) for key,data in rawOutlooks) print(\"gain\",column,\"=\",gain) print() return gain gains = [[x,findGain(x)] for x in ['Cuaca','Suhu','Kelembapan','Berangin']] DataFrame(gains) Data N Peluang 0 Berawan 4 0.285714 1 Cerah 5 0.357143 2 Hujan 5 0.357143 gain Cuaca = 0.2585210366587628 Data N Peluang 0 Dingin 4 0.285714 1 Panas 4 0.285714 2 Sejuk 6 0.428571 gain Suhu = 0.18385092540042125 Data N Peluang 0 Normal 7 0.5 1 Tinggi 7 0.5 gain Kelembapan = 0.3705065005495052 Data N Peluang 0 Benar 6 0.428571 1 Salah 8 0.571429 gain Berangin = 0.0059777114237739015 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 0 Cuaca 0.258521 1 Suhu 0.183851 2 Kelembapan 0.370507 3 Berangin 0.005978","title":"Tugas III"},{"location":"BAB III/#tugas-iii","text":"","title":"Tugas III"},{"location":"BAB III/#entropy-information-gain","text":"Algoritma pada metode ini menggunakan konsep dari entropi. Konsep Entropi yang digunakan untuk mengukur \u201cseberapa informatifnya\u201d sebuah node (yang biasanya disebut seberapa baiknya). Entropi(S) = 0, jika semua contoh pada S berada dalam kelas yang sama. Entroiy(S) = 1, jika jumlah contoh positif dan jumlah contoh negatif dalam S adalah sama. 0 < Entropi(S) < 1, jika jumlah contoh positif dan negatif dalam S tidak sama. Dimana: S adalah himpunan (dataset) kasus k adalah banyaknya partisi S pj adalah probabilitas yang di dapat dari Sum(Ya) dibagi Total Kasus. Setelah mendapat nilai entropi, pemilihan atribut dilakukan dengan nilai information gain terbesar. gain decision tree information gain Dimana: S = ruang (data) sample yang digunakan untuk training. A = atribut. |Si| = jumlah sample untuk nilai V. |S| = jumlah seluruh sample data. Entropi(Si) = entropy untuk sample-sample yang memiliki nilai i","title":"Entropy &amp; Information Gain"},{"location":"BAB III/#contoh","text":"Data yang telah ada pada Tabel dibawah akan digunakan untuk membentuk pohon keputusan dimana memiliku atribut-atribut seperti Cuaca, Suhu, Kelembaban, dan Berangin. Setiap atribut memiliki nilai. Sedangkan kelasnya ada pada kolom Main yaitu kelas \u201cTidak\u201d dan kelas \u201cYa\u201d. Kemudian data tersebut dianalisis; dataset tersebut memiliki 14 kasus yang terdiri 10 \u201cYa\u201d dan 4 \u201cTidak\u201d pada kolom Main. Kemudian hitung entropi dengan rumus seperti diatas. Entropi (S) = (-(10/14) x log2 (10/14) + (-(4/10) x log2 (4/10)) = 0.863120569 hasil perhitungan entropi Setelah mendapatkan entropi dari keseluruhan kasus, lakukan analisis pada setiap atribut dan nilai-nilainya dan hitung entropinya. analisis atribut, nilai, banyaknya kejadian, entropi dan gain Setelah mendapatkan nilai entropy, berikutnya hitung nilai information gain dari setiap variabel. Gain (Cuaca) = 0.863120569 \u2013 ((4/10) x 0 + (5/14) x 0.721928095 + (5/14) x 0.970950594) = 0.258521037","title":"Contoh"},{"location":"BAB III/#implementasi","text":"from pandas import* from math import log from sklearn.feature_selection import mutual_info_classif df = read_csv(\"BAB III.csv\", sep=';') df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Cuaca Suhu Kelembapan Berangin Main 0 Cerah Panas Tinggi Salah Tidak 1 Cerah Panas Tinggi Benar Tidak 2 Berawan Panas Tinggi Salah Ya 3 Hujan Sejuk Tinggi Salah Ya 4 Hujan Dingin Normal Salah Ya 5 Hujan Dingin Normal Benar Ya 6 Berawan Dingin Normal Benar Ya 7 Cerah Sejuk Tinggi Salah Tidak 8 Cerah Dingin Normal Salah Ya 9 Hujan Sejuk Normal Salah Ya 10 Cerah Sejuk Normal Benar Ya 11 Berawan Sejuk Tinggi Benar Ya 12 Berawan Panas Normal Salah Ya 13 Hujan Sejuk Tinggi Benar Tidak def findEntropy(column): rawGroups = df.groupby(column) targetGroups = [[key, len(data), len(data)/df[column].size] for key,data in rawGroups] targetGroups = DataFrame(targetGroups, columns=['Data', 'N', 'Peluang']) return sum([-x*log(x,2) for x in targetGroups['Peluang']]), targetGroups, rawGroups entropyTarget, groupTargets, _ = findEntropy('Main') print('entropy target =', entropyTarget) groupTargets entropy target = 0.863120568566631 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Data N Peluang 0 Tidak 4 0.285714 1 Ya 10 0.714286 def findGain(column): entropyOutlook, groupOutlooks, rawOutlooks = findEntropy(column) print(groupOutlooks) gain = entropyTarget-sum(len(data)/len(df)*sum(-x/len(data)*log(x/len(data),2) for x in data.groupby('Main').size()) for key,data in rawOutlooks) print(\"gain\",column,\"=\",gain) print() return gain gains = [[x,findGain(x)] for x in ['Cuaca','Suhu','Kelembapan','Berangin']] DataFrame(gains) Data N Peluang 0 Berawan 4 0.285714 1 Cerah 5 0.357143 2 Hujan 5 0.357143 gain Cuaca = 0.2585210366587628 Data N Peluang 0 Dingin 4 0.285714 1 Panas 4 0.285714 2 Sejuk 6 0.428571 gain Suhu = 0.18385092540042125 Data N Peluang 0 Normal 7 0.5 1 Tinggi 7 0.5 gain Kelembapan = 0.3705065005495052 Data N Peluang 0 Benar 6 0.428571 1 Salah 8 0.571429 gain Berangin = 0.0059777114237739015 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 0 Cuaca 0.258521 1 Suhu 0.183851 2 Kelembapan 0.370507 3 Berangin 0.005978","title":"Implementasi"}]}